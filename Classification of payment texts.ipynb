{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import collections\n",
    "import pymorphy2\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging, datetime, os\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#%config Application.log_level=\"INFO\"\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "warnings.simplefilter('ignore')\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_punctuation(s):\n",
    "    return ' '.join((re.sub(r'[№\"\\'-_/.:?!1234567890()%<>;,+#$&\\s+]', u' ', s)).split())\n",
    "\n",
    "def lemmatize(s):    \n",
    "    return ' '.join([morph.parse(w)[0].normal_form for w in s.split()])\n",
    "\n",
    "##Получаем слово из словаря по индексу\n",
    "\n",
    "def get_word_from_dict(dict_name,index):\n",
    "    return list(dict_name.keys())[list(dict_name.values()).index(int(index))]    \n",
    "\n",
    "# Создаем класс \"Мусор\"                   \n",
    "# n - количество топовых в классе слов, наличие которых проверяется в тексте\n",
    "# Если ни одного из этих слов нет, то текст отправляется в \"мусор\"\n",
    "\n",
    "def create_data_with_garbage(data,n):    \n",
    "    \n",
    "    #Создаем словари классов и слов:\n",
    "\n",
    "    class_indexes = {}\n",
    "    for i in range(len((set(data['CLASS'])))):\n",
    "        class_indexes[sorted(set(data['CLASS']))[i]] = i\n",
    "\n",
    "    cv = CountVectorizer(stop_words=stopwords)    \n",
    "    word_count_vector = cv.fit_transform(data['TEXT'].tolist())       \n",
    "    word_indexes = cv.vocabulary_\n",
    "\n",
    "    #Считаем сколько раз слова встречаются в каждом классе\n",
    "\n",
    "    class_word_count = np.empty((0,3))\n",
    "\n",
    "    for class_name in class_indexes:\n",
    "\n",
    "        text_class = data.loc[data['CLASS'] == class_name]['TEXT'].tolist()  \n",
    "\n",
    "        # Инициализируем и подгоняем CV:\n",
    "\n",
    "        cv = CountVectorizer(max_df = 0.5,min_df = 20,stop_words=stopwords)    \n",
    "        word_count_vector = cv.fit_transform(text_class)           \n",
    "\n",
    "        # Заполняем массивы для подсчета встерчаемости слов в классах:   \n",
    "\n",
    "        class_word_count = np.append(class_word_count,[[class_indexes.get(class_name),word_indexes.get(word),word_count_vector.sum(axis = 0)[0, idx]] for word, idx in cv.vocabulary_.items()],axis=0) \n",
    "    \n",
    "    # выделяем мусорный класс\n",
    "    \n",
    "    top_words = []\n",
    "    \n",
    "    data_new  = data.copy()\n",
    "    \n",
    "    for c in set(class_word_count.transpose()[0]):\n",
    "        \n",
    "        class_name = get_word_from_dict(class_indexes,c)    \n",
    "            \n",
    "        for clss,word,cnt in np.array(pd.DataFrame(class_word_count[class_word_count.transpose()[0] == c]).sort_values(by = [2],ascending = False))[:n]:            \n",
    "            word = get_word_from_dict(word_indexes,word) \n",
    "\n",
    "            top_words.append(word)\n",
    "    \n",
    "    top_words = set(top_words)       \n",
    "    \n",
    "    for index,row in data_new.iterrows():                              \n",
    "\n",
    "        if not top_words.isdisjoint(row['TEXT'].split(' ')):                \n",
    "            pass            \n",
    "        else:                \n",
    "            data_new.set_value(index,'CLASS','Мусор')\n",
    "            \n",
    "    return data_new      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исходные данные "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "with open('stopwords.txt','r', encoding='cp1251') as f:\n",
    "    stopwords_txt = f.read().splitlines() \n",
    "    \n",
    "with open('names.txt','r', encoding='cp1251') as f:\n",
    "    stopnames = f.read().splitlines()     \n",
    "\n",
    "stopwords = stopwords_txt + get_stop_words('ru')  + nltk.corpus.stopwords.words('russian') + stopnames    \n",
    "    \n",
    "#learning_file = 'naznach_for_24_classes_lemmatized.csv'   \n",
    "#training_file = 'naznach_for_24_classes_lemmatized.csv'\n",
    "\n",
    "#learning_file = 'naznach_for_24_classes_1_product_inn_v4_lemmatized.csv'   \n",
    "#training_file = 'naznach_for_24_classes_1_product_inn_v4_lemmatized.csv'\n",
    "\n",
    "#learning_file = 'naznach_for_24_classes_1_product_inn_v5_lemmatized.csv'   \n",
    "#training_file = 'naznach_for_24_classes_1_product_inn_v5_lemmatized.csv'\n",
    "\n",
    "learning_file = 'data_clean.csv'\n",
    "learning_file = 'data_clean.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Инициализация массива даных:\n",
    "    \n",
    "data = pd.read_csv(learning_file, encoding='utf-8', delimiter=';')\n",
    "data = data.dropna(subset=['TEXT'])\n",
    "data = data.drop_duplicates().reset_index(drop = True)\n",
    "data_new = create_data_with_garbage(data,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(data_new[data_new['CLASS']=='Мусор']['TEXT'].values[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new['TEXT'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "                                                 Мусор       0.75      0.89      0.82     88044\n",
      "                                          Автозапчасти       0.93      0.85      0.89    132987\n",
      "                                            Автомобили       0.65      0.60      0.62      2772\n",
      "                                       Бытовая техника       0.84      0.64      0.73     21583\n",
      "                                         Бытовая химия       0.99      0.99      0.99   2676928\n",
      "                                                   ГСМ       0.84      0.54      0.66     64235\n",
      "                                          Канцелярский       0.96      0.98      0.97   1061975\n",
      "                             Компьютерное оборудование       0.86      0.87      0.86     22330\n",
      "                                                Мебель       0.99      0.98      0.98   1420402\n",
      "                                           Медицинский       1.00      1.00      1.00   5416812\n",
      "                                          Оборудование       0.88      0.73      0.80     23228\n",
      "                                                Одежда       0.95      0.95      0.95    141858\n",
      "Оказание услуг по проведению лабораторных исследований       0.92      0.93      0.93     72812\n",
      "                                            Оргтехника       0.86      0.84      0.85     35050\n",
      "                                                Охрана       0.97      0.95      0.96    428722\n",
      "                                  Оценка условий труда       0.97      0.95      0.96    110316\n",
      "                                    Печатная продукция       0.85      0.83      0.84      5812\n",
      "                                               Питание       0.95      0.97      0.96    632787\n",
      "                                                Ремонт       0.93      0.85      0.89     40191\n",
      "                                            Сантехника       0.81      0.75      0.78     16427\n",
      "                                           Страхование       0.87      0.85      0.86     13750\n",
      "                                Строительные материалы       0.88      0.89      0.89    105842\n",
      "                                      Финансовый аудит       0.94      0.84      0.89      3838\n",
      "                                         Хозяйственный       0.84      0.84      0.84    244908\n",
      "                                         Электротовары       0.88      0.82      0.85    111795\n",
      "\n",
      "                                           avg / total       0.98      0.98      0.98  12895404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Обучение на всей выборке\n",
    "\n",
    "text_clf_lr = Pipeline([('tfidf',TfidfVectorizer(stop_words = stopwords)),('clf',OneVsRestClassifier(LinearSVC()))])\n",
    "text_clf_lr = text_clf_lr.fit(data_new['TEXT'],data_new['CLASS'])\n",
    "predicted_values = text_clf_lr.predict(data_new['TEXT'])\n",
    "print(metrics.classification_report(data_new['CLASS'],predicted_values,target_names = data_new['CLASS'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "                                                 Мусор       0.71      0.89      0.79     83011\n",
      "                                          Автозапчасти       0.84      0.85      0.85    121150\n",
      "                                            Автомобили       0.64      0.60      0.62      2660\n",
      "                                       Бытовая техника       0.79      0.67      0.72     19697\n",
      "                                         Бытовая химия       0.97      0.99      0.98   2632677\n",
      "                                                   ГСМ       0.80      0.54      0.64     61052\n",
      "                                          Канцелярский       0.86      0.98      0.92    948582\n",
      "                             Компьютерное оборудование       0.81      0.89      0.85     20785\n",
      "                                                Мебель       0.96      0.99      0.97   1373192\n",
      "                                           Медицинский       1.00      0.94      0.97   5774525\n",
      "                                          Оборудование       0.81      0.74      0.77     21223\n",
      "                                                Одежда       0.90      0.95      0.93    133850\n",
      "Оказание услуг по проведению лабораторных исследований       0.85      0.94      0.89     66343\n",
      "                                            Оргтехника       0.84      0.85      0.84     33527\n",
      "                                                Охрана       0.93      0.96      0.94    409389\n",
      "                                  Оценка условий труда       0.96      0.96      0.96    108029\n",
      "                                    Печатная продукция       0.83      0.85      0.84      5485\n",
      "                                               Питание       0.88      0.98      0.93    584639\n",
      "                                                Ремонт       0.83      0.87      0.85     35114\n",
      "                                            Сантехника       0.75      0.77      0.76     14824\n",
      "                                           Страхование       0.79      0.87      0.83     12270\n",
      "                                Строительные материалы       0.79      0.90      0.84     94614\n",
      "                                      Финансовый аудит       0.93      0.85      0.89      3774\n",
      "                                         Хозяйственный       0.80      0.85      0.82    231676\n",
      "                                         Электротовары       0.82      0.82      0.82    103316\n",
      "\n",
      "                                           avg / total       0.96      0.95      0.95  12895404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(training_file, encoding='utf-8', delimiter=';')\n",
    "data = data.dropna(subset=['TEXT'])\n",
    "data_new = create_data_with_garbage(data,50)\n",
    "data_new['PREDICTED'] = text_clf_lr.predict(data_new['TEXT'])\n",
    "print(metrics.classification_report(data_new['CLASS'],data_new['PREDICTED'],target_names = data_new['CLASS'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_new[data_new['PREDICTED']=='ГСМ'][['TEXT','PREDICTED']] \\\n",
    ".to_csv('sample_gsm_client_payments.csv',sep = ';',index=False,encoding='cp1251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new[data_new['PREDICTED']=='ГСМ'][['TEXT','PREDICTED']].sample(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классифицируем клиентов с несколькими продуками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('naznach_for_24_classes_v5.csv', encoding='utf-8', delimiter=';')\n",
    "data = data.rename(columns={'class':'CLASS','text':'TEXT'})\n",
    "data = data.dropna(subset=['TEXT'])\n",
    "data_new = create_data_with_garbage(data,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_new['PREDICTED'] = text_clf_lr.predict(data_new['TEXT'])\n",
    "print(metrics.classification_report(data_new['CLASS'],data_new['PREDICTED'],target_names = data_new['PREDICTED'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формируем выборку для кластеризации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_new[['INN','SUMMA','PREDICTED']].to_csv('clients_clustering_v5.csv',sep = ';',index=False,encoding='cp1251')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сэмпл данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_new[data_new['CLASS']=='Канцелярский'].sample(n=200).to_csv('data_sample_canc.csv',sep = ';',index=False,encoding='cp1251')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация текстов платежек случайных клиентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_file = 'new_part_test_data_lemmatized.csv'\n",
    "data_random_clients = pd.read_csv(training_file, encoding='utf-8', delimiter=';')\n",
    "data_random_clients = data_random_clients.dropna(subset=['text'])\n",
    "data_random_clients['PREDICTED'] = text_clf_lr.predict(data_random_clients['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "data_random_clients[data_random_clients['PREDICTED']!='Мусор'][['text','PREDICTED']].sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168390"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_random_clients['inn'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40765"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_random_clients[data_random_clients['PREDICTED']!='Мусор']['inn'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_random_clients['COUNT'] = 1\n",
    "\n",
    "data_random_clients_pivot = pd.pivot_table(data_random_clients,\n",
    "               values=['COUNT'],\n",
    "               index = ['inn','PREDICTED'],\n",
    "               aggfunc={\n",
    "                   'COUNT' : np.sum,\n",
    "               }).unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_not_garbage = [x for x in list(data_random_clients_pivot.columns) \n",
    "                      if 'Мусор' not in x                                                               \n",
    "                     ]\n",
    "\n",
    "label_garbage = [x for x in list(data_random_clients_pivot.columns) \n",
    "                      if 'Мусор' in x                                                               \n",
    "                     ]\n",
    "\n",
    "data_random_clients_pivot['SUM_NOT_GARBAGE'] = data_random_clients_pivot[labels_not_garbage].sum(axis=1) \n",
    "data_random_clients_pivot['GARBAGE'] = data_random_clients_pivot[label_garbage].values\n",
    "data_random_clients_pivot['GARBAGE %'] = data_random_clients_pivot['GARBAGE'] / (data_random_clients_pivot['SUM_NOT_GARBAGE'] + data_random_clients_pivot['GARBAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8628561583026206"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_random_clients_pivot['GARBAGE %'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select distinct inn from atb_segmen_tr_all_t\n",
      "Connecting...\n",
      "Getting data ... \n",
      "Downloaded 10,000 lines,       0sec. passed\n",
      "Downloaded 20,000 lines,       0sec. passed\n",
      "Downloaded 30,000 lines,       0sec. passed\n",
      "Downloaded 40,000 lines,       0sec. passed\n",
      "Downloaded 50,000 lines,       0sec. passed\n",
      "Downloaded 60,000 lines,       0sec. passed\n",
      "Downloaded 70,000 lines,       0sec. passed\n",
      "Downloaded 80,000 lines,       0sec. passed\n",
      "Downloaded 90,000 lines,       1sec. passed\n",
      "Downloaded 100,000 lines,       1sec. passed\n",
      "Downloaded 110,000 lines,       1sec. passed\n",
      "Downloaded 120,000 lines,       1sec. passed\n",
      "Downloaded 130,000 lines,       1sec. passed\n",
      "Downloaded 140,000 lines,       1sec. passed\n",
      "Downloaded 150,000 lines,       1sec. passed\n",
      "Downloaded 160,000 lines,       1sec. passed\n",
      "Downloaded 170,000 lines,       1sec. passed\n",
      "Downloaded 180,000 lines,       1sec. passed\n",
      "Downloaded 190,000 lines,       2sec. passed\n",
      "Downloaded 200,000 lines,       2sec. passed\n",
      "Downloaded 210,000 lines,       2sec. passed\n",
      "Downloaded 220,000 lines,       2sec. passed\n",
      "Downloaded 230,000 lines,       2sec. passed\n",
      "Downloaded 240,000 lines,       2sec. passed\n",
      "Downloaded 250,000 lines,       2sec. passed\n",
      "Downloaded 260,000 lines,       2sec. passed\n",
      "Downloaded 270,000 lines,       2sec. passed\n",
      "Downloaded 280,000 lines,       2sec. passed\n",
      "Downloaded 290,000 lines,       2sec. passed\n",
      "Downloaded 300,000 lines,       2sec. passed\n",
      "Downloaded 310,000 lines,       2sec. passed\n",
      "Downloaded 320,000 lines,       3sec. passed\n",
      "Downloaded 330,000 lines,       3sec. passed\n",
      "Downloaded 340,000 lines,       3sec. passed\n",
      "Downloaded 350,000 lines,       3sec. passed\n",
      "Downloaded 360,000 lines,       3sec. passed\n",
      "Downloaded 370,000 lines,       3sec. passed\n",
      "Downloaded 380,000 lines,       3sec. passed\n",
      "Downloaded 390,000 lines,       3sec. passed\n",
      "Downloaded 400,000 lines,       3sec. passed\n",
      "Downloaded 410,000 lines,       3sec. passed\n",
      "Downloaded 420,000 lines,       3sec. passed\n",
      "Downloaded 430,000 lines,       3sec. passed\n",
      "Downloaded 440,000 lines,       3sec. passed\n",
      "Downloaded 450,000 lines,       4sec. passed\n",
      "Downloaded 460,000 lines,       4sec. passed\n",
      "Downloaded 470,000 lines,       4sec. passed\n",
      "Downloaded 480,000 lines,       4sec. passed\n",
      "Downloaded 490,000 lines,       4sec. passed\n",
      "Downloaded 500,000 lines,       4sec. passed\n",
      "Downloaded 510,000 lines,       4sec. passed\n",
      "Downloaded 520,000 lines,       4sec. passed\n",
      "Downloaded 530,000 lines,       4sec. passed\n",
      "Downloaded 540,000 lines,       4sec. passed\n",
      "Downloaded 550,000 lines,       4sec. passed\n",
      "Downloaded 560,000 lines,       4sec. passed\n",
      "Downloaded 570,000 lines,       4sec. passed\n",
      "Downloaded 580,000 lines,       4sec. passed\n",
      "Downloaded 590,000 lines,       4sec. passed\n",
      "Downloaded 600,000 lines,       4sec. passed\n",
      "Downloaded 610,000 lines,       5sec. passed\n",
      "Downloaded 620,000 lines,       5sec. passed\n",
      "Downloaded 630,000 lines,       5sec. passed\n",
      "Downloaded 640,000 lines,       5sec. passed\n",
      "Downloaded 650,000 lines,       5sec. passed\n",
      "Downloaded 660,000 lines,       5sec. passed\n",
      "Downloaded 670,000 lines,       5sec. passed\n",
      "Downloaded 680,000 lines,       5sec. passed\n",
      "Downloaded 690,000 lines,       5sec. passed\n",
      "Downloaded 700,000 lines,       5sec. passed\n",
      "Downloaded 710,000 lines,       5sec. passed\n",
      "Downloaded 720,000 lines,       5sec. passed\n",
      "Downloaded 730,000 lines,       5sec. passed\n",
      "Downloaded 740,000 lines,       5sec. passed\n",
      "Downloaded 750,000 lines,       5sec. passed\n",
      "Downloaded 760,000 lines,       6sec. passed\n",
      "Downloaded 770,000 lines,       6sec. passed\n",
      "Downloaded 780,000 lines,       6sec. passed\n",
      "Downloaded 790,000 lines,       6sec. passed\n",
      "Downloaded 800,000 lines,       6sec. passed\n",
      "Downloaded 810,000 lines,       6sec. passed\n",
      "Downloaded 820,000 lines,       6sec. passed\n",
      "Downloaded 830,000 lines,       6sec. passed\n",
      "Downloaded 840,000 lines,       6sec. passed\n",
      "Downloaded 850,000 lines,       6sec. passed\n",
      "Downloaded 860,000 lines,       6sec. passed\n",
      "Downloaded 870,000 lines,       6sec. passed\n",
      "Downloaded 880,000 lines,       6sec. passed\n",
      "Downloaded 890,000 lines,       6sec. passed\n",
      "Downloaded 900,000 lines,       7sec. passed\n",
      "Downloaded 910,000 lines,       7sec. passed\n",
      "Downloaded 920,000 lines,       7sec. passed\n",
      "Downloaded 930,000 lines,       7sec. passed\n",
      "Downloaded 940,000 lines,       7sec. passed\n",
      "Downloaded 950,000 lines,       7sec. passed\n",
      "Downloaded 960,000 lines,       7sec. passed\n",
      "Downloaded 970,000 lines,       7sec. passed\n",
      "Downloaded 980,000 lines,       7sec. passed\n",
      "Downloaded 990,000 lines,       7sec. passed\n",
      "Downloaded 1,000,000 lines,       7sec. passed\n",
      "Downloaded 1,010,000 lines,       7sec. passed\n",
      "Downloaded 1,020,000 lines,       7sec. passed\n",
      "Downloaded 1,030,000 lines,       7sec. passed\n",
      "Downloaded 1,040,000 lines,       7sec. passed\n",
      "Downloaded 1,050,000 lines,       7sec. passed\n",
      "Downloaded 1,060,000 lines,       8sec. passed\n",
      "Downloaded 1,070,000 lines,       8sec. passed\n",
      "Downloaded 1,080,000 lines,       8sec. passed\n",
      "Downloaded 1,090,000 lines,       8sec. passed\n",
      "Downloaded 1,100,000 lines,       8sec. passed\n",
      "Downloaded 1,110,000 lines,       8sec. passed\n",
      "Downloaded 1,120,000 lines,       8sec. passed\n",
      "Downloaded 1,130,000 lines,       8sec. passed\n",
      "Downloaded 1,140,000 lines,       8sec. passed\n",
      "Downloaded 1,150,000 lines,       8sec. passed\n",
      "Downloaded 1,160,000 lines,       8sec. passed\n",
      "Downloaded 1,170,000 lines,       8sec. passed\n",
      "Downloaded 1,180,000 lines,       8sec. passed\n",
      "Downloaded 1,190,000 lines,       8sec. passed\n",
      "Downloaded 1,200,000 lines,       9sec. passed\n",
      "Downloaded 1,210,000 lines,       9sec. passed\n",
      "Downloaded 1,220,000 lines,       9sec. passed\n",
      "Downloaded 1,230,000 lines,       9sec. passed\n",
      "Downloaded 1,240,000 lines,       9sec. passed\n",
      "Downloaded 1,250,000 lines,       9sec. passed\n",
      "Downloaded 1,260,000 lines,       9sec. passed\n",
      "Downloaded 1,270,000 lines,       9sec. passed\n",
      "Downloaded 1,280,000 lines,       9sec. passed\n",
      "Downloaded 1,290,000 lines,       9sec. passed\n",
      "Downloaded 1,300,000 lines,       9sec. passed\n",
      "Downloaded 1,310,000 lines,       9sec. passed\n",
      "Downloaded 1,320,000 lines,      10sec. passed\n",
      "Downloaded 1,330,000 lines,      10sec. passed\n",
      "Downloaded 1,340,000 lines,      10sec. passed\n",
      "Downloaded 1,350,000 lines,      10sec. passed\n",
      "Downloaded 1,360,000 lines,      10sec. passed\n",
      "Downloaded 1,370,000 lines,      10sec. passed\n",
      "Downloaded 1,380,000 lines,      10sec. passed\n",
      "Downloaded 1,390,000 lines,      10sec. passed\n",
      "Downloaded 1,400,000 lines,      10sec. passed\n",
      "Downloaded 1,410,000 lines,      10sec. passed\n",
      "Downloaded 1,420,000 lines,      10sec. passed\n",
      "Downloaded 1,430,000 lines,      10sec. passed\n",
      "Downloaded 1,440,000 lines,      10sec. passed\n",
      "Downloaded 1,450,000 lines,      10sec. passed\n",
      "Downloaded 1,460,000 lines,      10sec. passed\n",
      "Downloaded 1,470,000 lines,      10sec. passed\n",
      "Downloaded 1,480,000 lines,      10sec. passed\n",
      "Downloaded 1,490,000 lines,      11sec. passed\n",
      "Downloaded 1,500,000 lines,      11sec. passed\n",
      "Downloaded 1,510,000 lines,      11sec. passed\n",
      "Downloaded 1,520,000 lines,      11sec. passed\n",
      "Downloaded 1,530,000 lines,      11sec. passed\n",
      "Downloaded 1,540,000 lines,      11sec. passed\n",
      "Downloaded 1,550,000 lines,      11sec. passed\n",
      "Downloaded 1,560,000 lines,      11sec. passed\n",
      "Downloaded 1,570,000 lines,      11sec. passed\n",
      "Downloaded 1,580,000 lines,      11sec. passed\n",
      "Downloaded 1,590,000 lines,      11sec. passed\n",
      "Downloaded 1,600,000 lines,      11sec. passed\n",
      "Downloaded 1,610,000 lines,      11sec. passed\n",
      "Downloaded 1,620,000 lines,      11sec. passed\n",
      "Downloaded 1,630,000 lines,      11sec. passed\n",
      "Downloaded 1,640,000 lines,      11sec. passed\n",
      "Downloaded 1,650,000 lines,      12sec. passed\n",
      "Downloaded 1,660,000 lines,      12sec. passed\n",
      "Downloaded 1,670,000 lines,      12sec. passed\n",
      "Downloaded 1,680,000 lines,      12sec. passed\n",
      "Downloaded 1,690,000 lines,      12sec. passed\n",
      "Downloaded 1,700,000 lines,      12sec. passed\n",
      "Downloaded 1,710,000 lines,      12sec. passed\n",
      "Downloaded 1,720,000 lines,      12sec. passed\n",
      "Downloaded 1,730,000 lines,      12sec. passed\n",
      "Downloaded 1,740,000 lines,      12sec. passed\n",
      "Downloaded 1,750,000 lines,      12sec. passed\n",
      "Downloaded 1,760,000 lines,      12sec. passed\n",
      "Downloaded 1,770,000 lines,      12sec. passed\n",
      "Downloaded 1,780,000 lines,      12sec. passed\n",
      "Downloaded 1,790,000 lines,      12sec. passed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1,800,000 lines,      12sec. passed\n",
      "Downloaded 1,810,000 lines,      13sec. passed\n",
      "Downloaded 1,820,000 lines,      13sec. passed\n",
      "Downloaded 1,830,000 lines,      13sec. passed\n",
      "Downloaded 1,840,000 lines,      13sec. passed\n",
      "Downloaded 1,850,000 lines,      13sec. passed\n",
      "Downloaded 1,860,000 lines,      13sec. passed\n",
      "Downloaded 1,870,000 lines,      13sec. passed\n",
      "Downloaded 1,880,000 lines,      13sec. passed\n",
      "Downloaded 1,890,000 lines,      13sec. passed\n",
      "Downloaded 1,900,000 lines,      13sec. passed\n",
      "Downloaded 1,910,000 lines,      13sec. passed\n",
      "Downloaded 1,920,000 lines,      13sec. passed\n",
      "Downloaded 1,930,000 lines,      13sec. passed\n",
      "Downloaded 1,940,000 lines,      13sec. passed\n",
      "Downloaded 1,950,000 lines,      13sec. passed\n",
      "Downloaded 1,960,000 lines,      14sec. passed\n",
      "Downloaded 1,970,000 lines,      14sec. passed\n",
      "Downloaded 1,980,000 lines,      14sec. passed\n",
      "Downloaded 1,990,000 lines,      14sec. passed\n",
      "Downloaded 2,000,000 lines,      14sec. passed\n",
      "Downloaded 2,010,000 lines,      14sec. passed\n",
      "Downloaded 2,020,000 lines,      14sec. passed\n",
      "Downloaded 2,030,000 lines,      14sec. passed\n",
      "Downloaded 2,040,000 lines,      14sec. passed\n",
      "Downloaded 2,050,000 lines,      14sec. passed\n",
      "Downloaded 2,060,000 lines,      14sec. passed\n",
      "Downloaded 2,070,000 lines,      14sec. passed\n",
      "Downloaded 2,080,000 lines,      14sec. passed\n",
      "Downloaded 2,090,000 lines,      14sec. passed\n",
      "Downloaded 2,100,000 lines,      14sec. passed\n",
      "Downloaded 2,110,000 lines,      14sec. passed\n",
      "Downloaded 2,120,000 lines,      15sec. passed\n",
      "Downloaded 2,130,000 lines,      15sec. passed\n",
      "Downloaded 2,140,000 lines,      15sec. passed\n",
      "Downloaded 2,150,000 lines,      15sec. passed\n",
      "Downloaded 2,160,000 lines,      15sec. passed\n",
      "Downloaded 2,170,000 lines,      15sec. passed\n",
      "Downloaded 2,180,000 lines,      15sec. passed\n",
      "Downloaded 2,190,000 lines,      15sec. passed\n",
      "Downloaded 2,200,000 lines,      15sec. passed\n",
      "Downloaded 2,210,000 lines,      15sec. passed\n",
      "Downloaded 2,220,000 lines,      15sec. passed\n",
      "Downloaded 2,230,000 lines,      15sec. passed\n",
      "Downloaded 2,240,000 lines,      15sec. passed\n",
      "Downloaded 2,250,000 lines,      15sec. passed\n",
      "Downloaded 2,260,000 lines,      15sec. passed\n",
      "Downloaded 2,270,000 lines,      16sec. passed\n",
      "Downloaded 2,280,000 lines,      16sec. passed\n",
      "Downloaded 2,290,000 lines,      16sec. passed\n",
      "Downloaded 2,300,000 lines,      16sec. passed\n",
      "Downloaded 2,310,000 lines,      16sec. passed\n",
      "Downloaded 2,320,000 lines,      16sec. passed\n",
      "Downloaded 2,330,000 lines,      16sec. passed\n",
      "Downloaded 2,340,000 lines,      16sec. passed\n",
      "Downloaded 2,350,000 lines,      16sec. passed\n",
      "Downloaded 2,360,000 lines,      16sec. passed\n",
      "Downloaded 2,370,000 lines,      16sec. passed\n",
      "Downloaded 2,380,000 lines,      16sec. passed\n",
      "Downloaded 2,390,000 lines,      16sec. passed\n",
      "Downloaded 2,400,000 lines,      16sec. passed\n",
      "Downloaded 2,410,000 lines,      16sec. passed\n",
      "Downloaded 2,420,000 lines,      17sec. passed\n",
      "Downloaded 2,430,000 lines,      17sec. passed\n",
      "Downloaded 2,440,000 lines,      17sec. passed\n",
      "Downloaded 2,450,000 lines,      17sec. passed\n",
      "Downloaded 2,460,000 lines,      17sec. passed\n",
      "Downloaded 2,470,000 lines,      17sec. passed\n",
      "Downloaded 2,480,000 lines,      17sec. passed\n",
      "Downloaded 2,490,000 lines,      17sec. passed\n",
      "Downloaded 2,500,000 lines,      17sec. passed\n",
      "Downloaded 2,510,000 lines,      17sec. passed\n",
      "Downloaded 2,520,000 lines,      17sec. passed\n",
      "Downloaded 2,530,000 lines,      17sec. passed\n",
      "Downloaded 2,540,000 lines,      17sec. passed\n",
      "Downloaded 2,550,000 lines,      17sec. passed\n",
      "Downloaded 2,560,000 lines,      17sec. passed\n",
      "Downloaded 2,570,000 lines,      17sec. passed\n",
      "Downloaded 2,580,000 lines,      17sec. passed\n",
      "Downloaded 2,590,000 lines,      18sec. passed\n",
      "Downloaded 2,600,000 lines,      18sec. passed\n",
      "Downloaded 2,610,000 lines,      18sec. passed\n",
      "Downloaded 2,620,000 lines,      18sec. passed\n",
      "Downloaded 2,630,000 lines,      18sec. passed\n",
      "Downloaded 2,640,000 lines,      18sec. passed\n",
      "Downloaded 2,650,000 lines,      18sec. passed\n",
      "Downloaded 2,660,000 lines,      18sec. passed\n",
      "Downloaded 2,670,000 lines,      18sec. passed\n",
      "Downloaded 2,680,000 lines,      18sec. passed\n",
      "Downloaded 2,690,000 lines,      18sec. passed\n",
      "Downloaded 2,700,000 lines,      18sec. passed\n",
      "Downloaded 2,710,000 lines,      19sec. passed\n",
      "Downloaded 2,720,000 lines,      19sec. passed\n",
      "Downloaded 2,730,000 lines,      19sec. passed\n",
      "Downloaded 2,740,000 lines,      19sec. passed\n",
      "Downloaded 2,750,000 lines,      19sec. passed\n",
      "Downloaded 2,760,000 lines,      19sec. passed\n",
      "Downloaded 2,770,000 lines,      19sec. passed\n",
      "Downloaded 2,780,000 lines,      19sec. passed\n",
      "Downloaded 2,790,000 lines,      19sec. passed\n",
      "Downloaded 2,800,000 lines,      19sec. passed\n",
      "Downloaded 2,810,000 lines,      19sec. passed\n",
      "Downloaded 2,820,000 lines,      19sec. passed\n",
      "Downloaded 2,830,000 lines,      19sec. passed\n",
      "Downloaded 2,840,000 lines,      20sec. passed\n",
      "Downloaded 2,850,000 lines,      20sec. passed\n",
      "Downloaded 2,860,000 lines,      20sec. passed\n",
      "Downloaded 2,870,000 lines,      20sec. passed\n",
      "Downloaded 2,880,000 lines,      20sec. passed\n",
      "Downloaded 2,890,000 lines,      20sec. passed\n",
      "Downloaded 2,900,000 lines,      20sec. passed\n",
      "Downloaded 2,910,000 lines,      20sec. passed\n",
      "Downloaded 2,920,000 lines,      20sec. passed\n",
      "Downloaded 2,930,000 lines,      20sec. passed\n",
      "Downloaded 2,940,000 lines,      20sec. passed\n"
     ]
    }
   ],
   "source": [
    "from loader import Loader\n",
    "import os, sys\n",
    "\n",
    "        \n",
    "file= 'tmp_select_from_triggers.csv'\n",
    "query = 'select distinct inn from atb_segmen_tr_all_t'\n",
    "print(query)\n",
    "\n",
    "data=Loader(True).save_csv(query, path=file, verbose=1)   \n",
    "data_triggers = pd.read_csv('tmp_select_from_triggers.csv', encoding='utf-8', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19903"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.merge(data_triggers,pd.DataFrame(data_random_clients[data_random_clients['PREDICTED']!='Мусор']['inn'].drop_duplicates()),on = 'inn')['inn'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40708"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.merge(data_triggers,pd.DataFrame(data_random_clients['inn'].drop_duplicates()),on = 'inn')['inn'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_1 = 'new_part_test_data.csv'\n",
    "data_random_clients_src = pd.read_csv(file_1, encoding='utf-8', delimiter=';')\n",
    "data_random_clients_src = data_random_clients_src.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_random_clients_src['PREDICTED'] = data_random_clients['PREDICTED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_payment_sample = data_random_clients[data_random_clients['inn'].isin([ 101000776  #питание\n",
    "                                                     ,27814792826#гсм\n",
    "                                                     ,27814973163#фин аудит\n",
    "                                                     ,27815395349#автомобили\n",
    "                                                     ,27815497012#электротовары\n",
    "                                                     ,27615946895#хозяйственный\n",
    "                                                    ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_payment_sample['text'] = data_random_clients_src[data_random_clients_src['inn'].isin([ 101000776  #питание\n",
    "                                                     ,27814792826#гсм\n",
    "                                                     ,27814973163#фин аудит\n",
    "                                                     ,27815395349#автомобили\n",
    "                                                     ,27815497012#электротовары\n",
    "                                                     ,27615946895#хозяйственный\n",
    "                                                    ])]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_payment_sample[['inn','text','PREDICTED']].sort_values(by = ['inn','PREDICTED']) \\\n",
    ".to_csv('sample_client_payments.csv',sep = ';',index=False,encoding='cp1251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация массива даных:\n",
    "    \n",
    "# data2 = pd.read_csv('naznach_for_24_classes_1_product_inn_v5_lemmatized.csv', encoding='utf-8', delimiter=';')\n",
    "# data2 = data.dropna(subset=['TEXT'])\n",
    "# #data = data.drop_duplicates().reset_index(drop = True)\n",
    "# data_new2 = create_data_with_garbage(data,100)\n",
    "\n",
    "# Обучение на всей выборке\n",
    "\n",
    "text_clf_lr2 = Pipeline([('tfidf',TfidfVectorizer(stop_words = stopwords,min_df = 1000,max_df = 0.6)),('clf',OneVsRestClassifier(LinearSVC()))])\n",
    "text_clf_lr2 = text_clf_lr2.fit(data_new2['TEXT'],data_new2['CLASS'])\n",
    "predicted_values2 = text_clf_lr2.predict(data_new2['TEXT'])\n",
    "print(metrics.classification_report(data_new2['CLASS'],predicted_values2,target_names = data_new2['CLASS'].unique()))\n",
    "\n",
    "#data_random_clients2 = pd.read_csv('new_part_test_data_lemmatized.csv', encoding='utf-8', delimiter=';')\n",
    "data_random_clients2 = pd.read_csv('new_part_test_data.csv', encoding='utf-8', delimiter=';')\n",
    "data_random_clients2 = data_random_clients2.dropna(subset=['text'])\n",
    "data_random_clients2['PREDICTED'] = text_clf_lr2.predict(data_random_clients2['text'])\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "data_random_clients2[data_random_clients2['PREDICTED']!='Мусор'][['text','PREDICTED']].sample(n=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bedf3b64920aa7e1c49377a911937112f9a54cf7a67efed16c9a0f6b95a532e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
