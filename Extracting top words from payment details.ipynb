{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import pymorphy2\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging, datetime, os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#%config Application.log_level=\"INFO\"\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "warnings.simplefilter('ignore')\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стопслова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for w in sorted(stopwords):\n",
    "#     print('u\\'%s\\','%format(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('stopwords.txt','w') as f:\n",
    "#     for item in stopwords:\n",
    "#         f.write('%s\\n' % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('stopwords.txt','r',encoding='cp1251') as f:\n",
    "#    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "with open('stopwords.txt','r', encoding='cp1251') as f:\n",
    "    stopwords_txt = f.read().splitlines() \n",
    "    \n",
    "with open('names.txt','r', encoding='cp1251') as f:\n",
    "    stopnames = f.read().splitlines()     \n",
    "\n",
    "stopwords = stopwords_txt + get_stop_words('ru')  + nltk.corpus.stopwords.words('russian') + stopnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_punctuation(s):\n",
    "    return ' '.join((re.sub(r'[№\"\\'-_/.:?!1234567890()%<>;,+#$&\\s+]', u' ', s)).split())\n",
    "\n",
    "def lemmatize(s):    \n",
    "    return ' '.join([morph.parse(w)[0].normal_form for w in s.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_from_dict(dict_name,index):\n",
    "    return list(dict_name.keys())[list(dict_name.values()).index(int(index))]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод результатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Топ слов по классам:\n",
    "def print_top_words_for_class(n,**kwargs):\n",
    "    \n",
    "    classname_p =  kwargs.get('classname_p',None)\n",
    "    \n",
    "    for c in set(class_word_count.transpose()[0]):\n",
    "        \n",
    "        class_name = get_word_from_dict(class_indexes,c)\n",
    "        \n",
    "        if classname_p is not None:\n",
    "            \n",
    "                if class_name == classname_p:\n",
    "        \n",
    "                    for clss,word,cnt in np.array(pd.DataFrame(class_word_count[class_word_count.transpose()[0] == c]).sort_values(by = [2],ascending = False))[:n]:\n",
    "                        class_name = get_word_from_dict(class_indexes,clss)\n",
    "                        word = get_word_from_dict(word_indexes,word) \n",
    "\n",
    "                        print(class_name,' | ',word,'|',cnt)\n",
    "        else:\n",
    "            \n",
    "            for clss,word,cnt in np.array(pd.DataFrame(class_word_count[class_word_count.transpose()[0] == c]).sort_values(by = [2],ascending = False))[:n]:\n",
    "                class_name = get_word_from_dict(class_indexes,clss)\n",
    "                word = get_word_from_dict(word_indexes,word) \n",
    "\n",
    "                print(class_name,' | ',word,'|',cnt)\n",
    "\n",
    "#Больше всего раз встречающиеся слова по классам:            \n",
    "def print_most_common_word_by_class():        \n",
    "    for w in np.array(pd_class_word_count):                    \n",
    "        class_name = get_word_from_dict(class_indexes,w[0])\n",
    "        word = get_word_from_dict(word_indexes,w[1]) \n",
    "        print(class_name,' | ',word,' | ',w[2])    \n",
    "\n",
    "#Больше всего раз встречающиеся слова в целом:        \n",
    "def print_most_common_words(n,**kwargs):\n",
    "    for w in np.array(pd_class_word_count[[1,3]].drop_duplicates().sort_values(by = [3],ascending = False))[:n]:                    \n",
    "        word = get_word_from_dict(word_indexes,w[0]) \n",
    "        \n",
    "        mode =  kwargs.get('mode',None)\n",
    "        \n",
    "        if mode is not None:\n",
    "            print(word,' | ',w[1])                \n",
    "        else:\n",
    "            print('u\\'%s\\','%format(word))\n",
    "\n",
    "#Уникальные ключевые слова классов:        \n",
    "def print_unique_words(**kwargs):\n",
    "    \n",
    "    classname_p =  kwargs.get('classname_p',None)\n",
    "\n",
    "    if classname_p is not None:\n",
    "\n",
    "        for w in [ w for w in np.array(pd.DataFrame(keywords_filtered).sort_values(by = [0,1])) if get_word_from_dict(class_indexes,w[0]) == classname_p]:                    \n",
    "\n",
    "            class_name = get_word_from_dict(class_indexes,w[0])\n",
    "            word = get_word_from_dict(word_indexes,w[1])                                     \n",
    "            print(class_name,' | ',word)\n",
    "\n",
    "    else:\n",
    "\n",
    "         for w in np.array(pd.DataFrame(keywords_filtered).sort_values(by = [0,1])):                    \n",
    "\n",
    "            class_name = get_word_from_dict(class_indexes,w[0])\n",
    "            word = get_word_from_dict(word_indexes,w[1])                 \n",
    "            print(class_name,' | ',word)\n",
    "\n",
    "#Слова, которые являются ключевыми в нескольких классах:\n",
    "def print_midex_words():\n",
    "    for w in np.array(pd.DataFrame(keywords_mixed).sort_values(by = [1])):                    \n",
    "        class_name = get_word_from_dict(class_indexes,w[0])\n",
    "        word = get_word_from_dict(word_indexes,w[1]) \n",
    "        print(class_name,' | ',word)    \n",
    "\n",
    "#Мусорные слова - встречаются в большом количестве классов (больше чем в 70% от общего количества классов):    \n",
    "def print_garbage_words():\n",
    "    for w in np.array(pd.DataFrame(keywords_garbage).sort_values(by = [0])):                    \n",
    "        word = get_word_from_dict(word_indexes,w) \n",
    "        print('u\\'%s\\','%format(word))\n",
    "\n",
    "#Слова, которые появляются в большом моличестве классов        \n",
    "def print_words_often_apper_in_classes():\n",
    "    for word,count in keywords_all_counter.most_common():\n",
    "        print(get_word_from_dict(word_indexes,word),' | ',count)\n",
    "\n",
    "#Таблица со всеми классами и количеством одинаковых ключевых слов в них         \n",
    "def print_table_with_same_words_for_classes():\n",
    "    #Считаем количество одинаковых ключевых слов в классах:\n",
    "\n",
    "    def get_index_csw(i,j):\n",
    "\n",
    "        it = 0\n",
    "        for w in classes_same_words:\n",
    "            if w[0] == i and w[1] == j:\n",
    "                return it\n",
    "                break\n",
    "            it += 1   \n",
    "        \n",
    "    classes_same_words = np.empty((0,3))\n",
    "\n",
    "    for i in list(class_indexes.values()):\n",
    "        for j in list(class_indexes.values()):\n",
    "            if i != j:\n",
    "                classes_same_words = np.append(classes_same_words,[[i,j,0]],axis = 0)       \n",
    "\n",
    "    for i in list(class_indexes.values()):\n",
    "        for w in keywords_mixed[np.array(keywords_mixed).transpose()[0] == i]:\n",
    "            for c in keywords_mixed[np.array(keywords_mixed).transpose()[1] == w[1]]:\n",
    "                if i != c[0]:                \n",
    "                    classes_same_words[get_index_csw(i,c[0])][2] = classes_same_words[get_index_csw(i,c[0])][2] + 1   \n",
    "\n",
    "    #Считаем количество ключевых слов в каждом классе:                \n",
    "\n",
    "    pd_class_key_word_sum =  pd.DataFrame(class_word_count[:,:2])\n",
    "    pd_class_key_word_sum[2] = pd_class_key_word_sum[1].groupby(pd_class_key_word_sum[0]).transform('count')\n",
    "    pd_class_key_word_sum = pd_class_key_word_sum.drop([1],axis=1).drop_duplicates().reset_index(drop=True)  \n",
    "\n",
    "    for w in np.array(pd.DataFrame(classes_same_words).sort_values(by = [0,2],ascending = False)):   \n",
    "        print(get_word_from_dict(class_indexes,w[0]),'(',int(pd_class_key_word_sum[pd_class_key_word_sum[0] == w[0]][2].iat[0]),')','|',get_word_from_dict(class_indexes,w[1]),'(',int(pd_class_key_word_sum[pd_class_key_word_sum[0] == w[1]][2].iat[0]),')','|',int(w[2]))                        \n",
    "        \n",
    "#Получаем одинаковые слова в классах       \n",
    "def get_same_words(class1,class2,**kwargs):\n",
    "        \n",
    "    class1_words = keywords_mixed[np.array(keywords_mixed).transpose()[0] == class_indexes.get(class1)][:,1:]\n",
    "    class2_words = keywords_mixed[np.array(keywords_mixed).transpose()[0] == class_indexes.get(class2)][:,1:]            \n",
    "\n",
    "    for w in np.intersect1d(class1_words,class2_words):\n",
    "        \n",
    "        mode =  kwargs.get('mode',None)\n",
    "        \n",
    "        if mode is not None:\n",
    "            print('u\\'%s\\','%format(get_word_from_dict(word_indexes,w)))                                    \n",
    "        else:\n",
    "            print(get_word_from_dict(word_indexes,w))\n",
    "            \n",
    "# Создаем класс \"Мусор\"                   \n",
    "# n - количество топовых в классе слов, наличие которых проверяется в тексте\n",
    "# Если ни одного из этих слов нет, то текст отправляется в \"мусор\"\n",
    "def create_data_with_garbage(data,n):        \n",
    "    \n",
    "    top_words = []\n",
    "    \n",
    "    data_new  = data.copy()\n",
    "    \n",
    "    for c in set(class_word_count.transpose()[0]):\n",
    "        \n",
    "        class_name = get_word_from_dict(class_indexes,c)    \n",
    "            \n",
    "        for clss,word,cnt in np.array(pd.DataFrame(class_word_count[class_word_count.transpose()[0] == c]).sort_values(by = [2],ascending = False))[:n]:            \n",
    "            word = get_word_from_dict(word_indexes,word) \n",
    "\n",
    "            top_words.append(word)\n",
    "    \n",
    "    top_words = set(top_words)       \n",
    "    \n",
    "    for index,row in data_new.iterrows():                              \n",
    "\n",
    "        if not top_words.isdisjoint(row['TEXT'].split(' ')):                \n",
    "            pass            \n",
    "        else:                \n",
    "            data_new.set_value(index,'CLASS','Мусор')\n",
    "            \n",
    "    return data_new            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выделение ключевых слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Инициализация массива даных:\n",
    "    \n",
    "data = pd.read_csv('data_clean.csv', encoding='cp1251', delimiter=';')\n",
    "data = data.dropna(subset=['TEXT'])\n",
    "#data = data.drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "#Создаем словари классов и слов:\n",
    "\n",
    "class_indexes = {}\n",
    "for i in range(len((set(data['CLASS'])))):\n",
    "    class_indexes[sorted(set(data['CLASS']))[i]] = i\n",
    "\n",
    "cv = CountVectorizer(stop_words=stopwords)    \n",
    "word_count_vector = cv.fit_transform(data['TEXT'].tolist())       \n",
    "word_indexes = cv.vocabulary_\n",
    "\n",
    "#Считаем сколько раз слова встречаются в каждом классе\n",
    "\n",
    "class_word_count = np.empty((0,3))\n",
    "\n",
    "for class_name in class_indexes:\n",
    "        \n",
    "    text_class = data.loc[data['CLASS'] == class_name]['TEXT'].tolist()  \n",
    "    \n",
    "    # Инициализируем и подгоняем CV:\n",
    "        \n",
    "    cv = CountVectorizer(max_df = 0.5,min_df = 20,stop_words=stopwords)    \n",
    "    word_count_vector = cv.fit_transform(text_class)           \n",
    "    \n",
    "    # Заполняем массивы для подсчета встерчаемости слов в классах:   \n",
    "    \n",
    "    class_word_count = np.append(class_word_count,[[class_indexes.get(class_name),word_indexes.get(word),word_count_vector.sum(axis = 0)[0, idx]] for word, idx in cv.vocabulary_.items()],axis=0) \n",
    "    \n",
    "#Делим ключевые слова на группы \"Уникальные\", \"Перемешанные\" и \"Мусорные\" \n",
    "\n",
    "keywords_filtered = np.empty((0,2))\n",
    "keywords_mixed = np.empty((0,2))\n",
    "keywords_garbage = np.empty((0,1))\n",
    "\n",
    "class_word_counter = collections.Counter(class_word_count.transpose()[1])\n",
    "    \n",
    "for word in set(class_word_count.transpose()[1]):\n",
    "    \n",
    "    class_word = class_word_count[class_word_count.transpose()[1] == word][:,:2]\n",
    "    \n",
    "    if len(class_word) == 1:\n",
    "        \n",
    "        keywords_filtered = np.append(keywords_filtered,class_word,axis=0)   \n",
    "    \n",
    "    elif len(class_word) <= int(max(class_word_counter.values())*0.4):\n",
    "    \n",
    "        keywords_mixed = np.append(keywords_mixed,class_word,axis=0) \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        keywords_garbage = np.append(keywords_garbage,class_word[0][1])   \n",
    "        \n",
    "pd_class_word_count =  pd.DataFrame(class_word_count).sort_values(by = [2],ascending = False)\n",
    "pd_class_word_count[3] = pd_class_word_count[2].groupby(pd_class_word_count[1]).transform('sum')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print_top_words_for_class(50,classname_p = 'Электротовары')\n",
    "# get_same_words('Электротовары','Страхование',mode=1)\n",
    "# print_table_with_same_words_for_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = 'naznach_for_24_classes_1_product_inn_v5.csv'\n",
    "file_path_new = 'naznach_for_24_classes_1_product_inn_v5_lemmatized.csv'\n",
    "\n",
    "data = pd.read_csv(file_path, encoding='cp1251', delimiter=';')\n",
    "data['TEXT'] = data['TEXT'].apply(lambda text : lemmatize(delete_punctuation(text.lower())))\n",
    "data = data.dropna(subset=['TEXT'])\n",
    "data.to_csv(file_path_new,sep = ';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#построчная запись в файл\n",
    "\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "file_path = 'naznach_for_24_classes_1_product_inn_v5.csv'\n",
    "file_path_new = 'naznach_for_24_classes_1_product_inn_v5_lemmatized.csv'\n",
    "\n",
    "with open(file_path,mode='r') as old_file:\n",
    "    \n",
    "    with open(file_path_new,mode='w') as new_file:\n",
    "    \n",
    "        old_file_reader = csv.DictReader(old_file)\n",
    "        \n",
    "        new_file_writer = csv.writer(new_file,delimiter=';')\n",
    "\n",
    "        for row in old_file_reader:\n",
    "            new_file_writer.writerow([lemmatize(delete_punctuation(str(row).lower()))])                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = 'naznach_for_24_classes_1_product_inn_v5.csv'\n",
    "file_path_new = 'naznach_for_24_classes_1_product_inn_v5_lemmatized.csv'\n",
    "\n",
    "data = pd.read_csv(file_path, encoding='cp1251', delimiter=';')\n",
    "data_new = pd.read_csv(file_path_new, encoding='utf-8', delimiter=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bedf3b64920aa7e1c49377a911937112f9a54cf7a67efed16c9a0f6b95a532e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
